{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38fffbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert:\n",
      "Notice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Revathy\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Test Accuracy: 0.71\n",
      "Epoch [2/5] - Test Accuracy: 0.71\n",
      "Epoch [3/5] - Test Accuracy: 0.71\n",
      "Epoch [4/5] - Test Accuracy: 0.71\n",
      "Epoch [5/5] - Test Accuracy: 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 71.43\n",
      "Precision: 51.020\n",
      "Recall: 71.429\n",
      "F1 Score: 59.524\n",
      "[[10  0]\n",
      " [ 4  0]]\n",
      "Breach\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Revathy\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Test Accuracy: 0.93\n",
      "Epoch [2/5] - Test Accuracy: 0.93\n",
      "Epoch [3/5] - Test Accuracy: 0.93\n",
      "Epoch [4/5] - Test Accuracy: 0.93\n",
      "Epoch [5/5] - Test Accuracy: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 92.86\n",
      "Precision: 86.224\n",
      "Recall: 92.857\n",
      "F1 Score: 89.418\n",
      "[[13  0]\n",
      " [ 1  0]]\n",
      "DataProcessing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Revathy\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Test Accuracy: 0.79\n",
      "Epoch [2/5] - Test Accuracy: 0.79\n",
      "Epoch [3/5] - Test Accuracy: 0.79\n",
      "Epoch [4/5] - Test Accuracy: 0.79\n",
      "Epoch [5/5] - Test Accuracy: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 78.57\n",
      "Precision: 61.735\n",
      "Recall: 78.571\n",
      "F1 Score: 69.143\n",
      "[[11  0]\n",
      " [ 3  0]]\n",
      "Security\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Revathy\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Test Accuracy: 0.93\n",
      "Epoch [2/5] - Test Accuracy: 0.93\n",
      "Epoch [3/5] - Test Accuracy: 0.93\n",
      "Epoch [4/5] - Test Accuracy: 0.93\n",
      "Epoch [5/5] - Test Accuracy: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 92.86\n",
      "Precision: 86.224\n",
      "Recall: 92.857\n",
      "F1 Score: 89.418\n",
      "[[13  0]\n",
      " [ 1  0]]\n",
      "Complaint/Request\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Revathy\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Test Accuracy: 0.93\n",
      "Epoch [2/5] - Test Accuracy: 0.93\n",
      "Epoch [3/5] - Test Accuracy: 0.93\n",
      "Epoch [4/5] - Test Accuracy: 0.93\n",
      "Epoch [5/5] - Test Accuracy: 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathy\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 92.86\n",
      "Precision: 86.224\n",
      "Recall: 92.857\n",
      "F1 Score: 89.418\n",
      "[[13  0]\n",
      " [ 1  0]]\n",
      "UserParticipation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\Revathy\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - Test Accuracy: 0.71\n",
      "Epoch [2/5] - Test Accuracy: 0.71\n",
      "Epoch [3/5] - Test Accuracy: 0.64\n",
      "Epoch [4/5] - Test Accuracy: 0.79\n",
      "Epoch [5/5] - Test Accuracy: 0.86\n",
      "Final Test Accuracy: 85.71\n",
      "Precision: 85.714\n",
      "Recall: 85.714\n",
      "F1 Score: 85.714\n",
      "[[9 1]\n",
      " [1 3]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"GPT.csv\", encoding='ANSI')\n",
    "security_class = ','.join(data['Category'].unique())\n",
    "security_class = set(security_class.split(\",\"))\n",
    "df = data.copy()\n",
    "\n",
    "for s in security_class:\n",
    "    df[s] = 0\n",
    "    df.loc[df['Category'].str.contains(s, regex=False), s] = 1\n",
    "\n",
    "Round = 3  \n",
    "\n",
    "fields = ['Notice', 'Breach', 'DataProcessing', 'Security', 'Complaint/Request', 'UserParticipation']\n",
    "\n",
    "print('Bert:')\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 5  \n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "\n",
    "for fold in fields:\n",
    "    print(fold)\n",
    "    train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df[fold])\n",
    "\n",
    "    # Load the BERT tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(security_class))\n",
    "\n",
    "    # Tokenize the text data\n",
    "    train_encodings = tokenizer(list(train_data['Article Text']), truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(list(test_data['Article Text']), truncation=True, padding=True)\n",
    "\n",
    "    # Create PyTorch DataLoader for training and testing\n",
    "    train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),\n",
    "                                  torch.tensor(train_encodings['attention_mask']),\n",
    "                                  torch.tensor(train_data[fold].values))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']),\n",
    "                                 torch.tensor(test_encodings['attention_mask']),\n",
    "                                 torch.tensor(test_data[fold].values))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop with hyperparameter tuning\n",
    "    best_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1]}\n",
    "            labels = batch[2]\n",
    "            outputs = model(**inputs)\n",
    "            loss = loss_fn(outputs.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate the model after each epoch on the validation set\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                inputs = {'input_ids': batch[0],\n",
    "                          'attention_mask': batch[1]}\n",
    "                labels = batch[2]\n",
    "                outputs = model(**inputs)\n",
    "                _, predicted = torch.max(outputs.logits, 1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += len(labels)\n",
    "\n",
    "        accuracy = total_correct / total_samples\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}] - Test Accuracy: {accuracy:.2f}')\n",
    "\n",
    "        # Keep track of the best model based on validation accuracy\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Load the best model state for testing\n",
    "    model.load_state_dict(best_model_state)\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs = {'input_ids': batch[0],\n",
    "                      'attention_mask': batch[1]}\n",
    "            labels = batch[2]\n",
    "            outputs = model(**inputs)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            true_labels.extend(labels.tolist())\n",
    "            predicted_labels.extend(predicted.tolist())\n",
    "\n",
    "    accuracy = (sum([1 for i, j in zip(true_labels, predicted_labels) if i == j]) / len(true_labels))*100\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted') * 100\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted') * 100\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted') * 100\n",
    "    \n",
    "    print(f'Final Test Accuracy: {accuracy:.2f}')\n",
    "    print(f'Precision: {precision:.{Round}f}')\n",
    "    print(f'Recall: {recall:.{Round}f}')\n",
    "    print(f'F1 Score: {f1:.{Round}f}')\n",
    "    \n",
    "    # Calculate and plot the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    print(cm)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
